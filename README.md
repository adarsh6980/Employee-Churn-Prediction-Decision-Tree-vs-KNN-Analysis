# ğŸ¯ Employee Churn Prediction - Decision Tree vs KNN

> **A Machine Learning Approach to HR Retention Strategy**  
> Predicting employee attrition using Decision Trees and K-Nearest Neighbors algorithms with 84.81% accuracy.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0%2B-orange)](https://scikit-learn.org/)
[![Status: Complete](https://img.shields.io/badge/Status-Complete-brightgreen)](https://github.com)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Installation & Setup](#installation--setup)
- [Models Overview](#models-overview)
- [Key Results](#key-results)
- [Data Exploration](#data-exploration)
- [Model Performance](#model-performance)
- [Business Insights](#business-insights)
- [Usage Guide](#usage-guide)
- [Results & Recommendations](#results--recommendations)
- [Tech Stack](#tech-stack)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

**Problem Statement:** Predict which employees are at risk of leaving the organization to enable proactive retention strategies.

**Business Impact:** Employee turnover costs organizations ~200% of annual salary per employee. Identifying at-risk employees early can save **$40,000-$100,000+ per prevented departure**.

**Solution:** Compare two machine learning algorithms:
- **Decision Tree**: Interpretable with clear decision rules
- **K-Nearest Neighbors**: High accuracy with 84.81% performance

**Key Achievement:** Successfully identified top 5 churn predictors accounting for 40% of decision-making:
1. Total Working Years (16.5%)
2. Monthly Income (13.6%)
3. Age (10.0%)
4. Overtime (7.9%)
5. Stock Options (7.6%)

---

## ğŸ“Š Dataset

**IBM HR Analytics Employee Attrition Dataset** ([Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset))

| Property | Value |
|----------|-------|
| **Total Employees** | 1,470 |
| **Features** | 34 â†’ 51 (after encoding) |
| **Target Variable** | Attrition (Binary) |
| **Retained** | 1,233 (83.9%) |
| **Churned** | 237 (16.1%) |
| **Missing Values** | 0 |

**Key Statistics:**
- Age: 18-60 years (Mean: 36.9)
- Monthly Income: $1,009-$19,999 (Mean: $6,503)
- Tenure: 0-40 years (Mean: 7.0)
- Total Working Years: 0-40 years (Mean: 11.3)

---

## ğŸ“ Project Structure

```
employee-churn-project/
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv      # Original dataset
â”‚   â”œâ”€â”€ X_features.csv                             # Preprocessed features (1470Ã—51)
â”‚   â””â”€â”€ y_target.csv                               # Target variable
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/
â”‚   â”œâ”€â”€ 1_data_exploration.ipynb                   # EDA & visualization
â”‚   â”œâ”€â”€ 2_decision_tree_analysis.ipynb             # DT model training
â”‚   â””â”€â”€ 3_knn_analysis.ipynb                       # KNN model & comparison
â”‚
â”œâ”€â”€ ğŸ“‚ results/
â”‚   â”œâ”€â”€ ğŸ“‚ plots/                                  # 15 high-resolution visualizations
â”‚   â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”‚   â”œâ”€â”€ decision_tree_model.pkl
â”‚   â”‚   â”œâ”€â”€ knn_model.pkl
â”‚   â”‚   â””â”€â”€ scaler.pkl
â”‚   â””â”€â”€ ğŸ“‚ csv/
â”‚       â”œâ”€â”€ dt_evaluation_results.csv
â”‚       â”œâ”€â”€ model_comparison_results.csv
â”‚       â””â”€â”€ dt_feature_importance.csv
â”‚
â”œâ”€â”€ ğŸ“‚ report/
â”‚   â””â”€â”€ Employee_Churn_Report.pdf                  # Complete analysis report
â”‚
â”œâ”€â”€ ğŸ“„ README.md                                   # This file
â”œâ”€â”€ ğŸ“„ requirements.txt                            # Python dependencies
â””â”€â”€ ğŸ“„ .gitignore                                  # Git ignore rules
```

---

## ğŸš€ Installation & Setup

### Prerequisites
- Python 3.8+
- pip or conda
- Git

### Step 1: Clone Repository
```bash
git clone https://github.com/yourusername/employee-churn-prediction.git
cd employee-churn-project
```

### Step 2: Create Virtual Environment
```bash
# Using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# OR using conda
conda create -n churn-env python=3.8
conda activate churn-env
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Download Dataset
```bash
# Download from Kaggle: 
# https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset
# Place in ./data/ folder as: WA_Fn-UseC_-HR-Employee-Attrition.csv
```

### Step 5: Run Notebooks (in order)
```bash
jupyter notebook notebooks/1_data_exploration.ipynb
jupyter notebook notebooks/2_decision_tree_analysis.ipynb
jupyter notebook notebooks/3_knn_analysis.ipynb
```

---

## ğŸ§  Models Overview

### Model 1: Decision Tree Classifier

**Configuration:**
```python
DecisionTreeClassifier(
    random_state=42,           # Reproducibility
    max_depth=10,              # Prevents overfitting
    min_samples_split=10,      # Requires 10+ samples per split
    min_samples_leaf=5,        # Terminal nodes: 5+ samples
    criterion='gini'           # Split criterion
)
```

**Strengths:**
- âœ… Interpretable decision rules
- âœ… Clear feature importance rankings
- âœ… Fast predictions (milliseconds)
- âœ… No feature scaling required

**Weaknesses:**
- âŒ Lower recall (26.76%) - misses many churners
- âŒ Lower ROC-AUC (0.5580)

---

### Model 2: K-Nearest Neighbors (KNN)

**Configuration:**
```python
KNeighborsClassifier(
    n_neighbors=11,            # Optimal K from grid search
    weights='distance',        # Closer neighbors weighted higher
    metric='euclidean',        # Euclidean distance in scaled space
    algorithm='auto'           # Automatic algorithm selection
)
```

**Data Preprocessing:**
```python
scaler = StandardScaler()      # Feature scaling (CRITICAL for KNN)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**Strengths:**
- âœ… Highest accuracy (84.81%)
- âœ… Best ROC-AUC (0.7494)
- âœ… Good precision (70%)
- âœ… Better generalization

**Weaknesses:**
- âŒ Low recall (9.86%) - conservative predictions
- âŒ Black box - no interpretability
- âŒ Slower predictions (must compare with all training data)

---

## ğŸ“Š Key Results

### Performance Comparison

| Metric | Decision Tree | KNN | Winner |
|--------|---------------|-----|--------|
| **Accuracy** | 80.05% | **84.81%** âœ… | KNN |
| **Precision** | 28.00% | **70.00%** âœ… | KNN |
| **Recall** | **26.76%** âœ… | 9.86% | Decision Tree |
| **F1-Score** | **0.3016** âœ… | 0.1730 | Decision Tree |
| **ROC-AUC** | 0.5580 | **0.7494** âœ… | KNN |
| **CV Mean** | 0.8222 | **0.8513** âœ… | KNN |

### Confusion Matrices

**Decision Tree:**
```
                    Predicted Retained  Predicted Churned
Actual Retained              334                12
Actual Churned                52                19
```
- Specificity: 96.5% (excellent at identifying retained)
- Sensitivity: 26.8% (struggles with churners)

**KNN:**
```
                    Predicted Retained  Predicted Churned
Actual Retained              367                 0
Actual Churned                64                 7
```
- Specificity: 100% (perfect at retained detection!)
- Sensitivity: 9.9% (too conservative)

---

## ğŸ“ˆ Data Exploration

### Plot 1: Target Distribution
![Target Distribution](01_target_distribution.jpg)

**Insight:** Realistic 84%-16% class imbalance requires stratified sampling to prevent model bias toward predicting retention.

---

### Plot 2: Age vs Attrition
![Age vs Attrition](02_age_vs_attrition.jpg)

**Key Finding:**
- Mean age (Churned): 33.4 years
- Mean age (Retained): 37.6 years
- **Difference: 4.2 years**

Younger employees (25-35) show **significantly higher churn**.

---

### Plot 3: Tenure vs Attrition
![Tenure vs Attrition](03_tenure_vs_attrition.jpg)

**Critical Pattern:**
- Mean tenure (Churned): 5.1 years
- Mean tenure (Retained): 7.5 years
- **40% of churners leave within <2 years**

**The first 2 years are a critical retention window!**

---

### Plot 4: Correlation Heatmap
![Correlation Heatmap](04_correlation_heatmap.jpg)

**Top Negative Correlations (Protective Factors):**
- Total Working Years: -0.17 âœ…
- Job Level: -0.17 âœ…
- Monthly Income: -0.16 âœ…
- Years at Company: -0.13 âœ…

Experience, seniority, compensation, and tenure all **reduce churn risk**.

---

## ğŸ“Š Model Performance

### Feature Importance (Decision Tree)
![Feature Importance](06_dt_feature_importance.jpg)

**Top 10 Predictors:**
| Rank | Feature | Importance |
|------|---------|-----------|
| 1 | Total Working Years | 16.5% |
| 2 | Monthly Income | 13.6% |
| 3 | Age | 10.0% |
| 4 | Overtime | 7.9% |
| 5 | Stock Options | 7.6% |
| 6 | Relationship Satisfaction | 5.1% |
| 7 | Environment Satisfaction | 4.1% |
| 8 | Daily Rate | 3.8% |
| 9 | Companies Worked | 3.5% |
| 10 | Hourly Rate | 3.3% |

---

### Decision Tree: Cross-Validation Scores
![DT CV Scores](8_dt_cv_scores.jpg)

**Result:** Mean: 0.8222 (Â±0.0130) - Highly stable, no overfitting detected.

---

### Decision Tree: Confusion Matrix
![DT Confusion Matrix](9_dt_confusion_matrix.jpg)

**Analysis:** Strong at identifying retained employees but misses many actual churners.

---

### Decision Tree: ROC Curve
![DT ROC Curve](10_dt_roc_curve.jpg)

**ROC-AUC: 0.5580** - Moderate discrimination, slightly better than random guessing.

---

### Decision Tree: Structure (First 3 Levels)
![DT Tree Structure](7_dt_tree_visualization.jpg)

**Key Decision Path:**
1. **Root**: TotalWorkingYears < 1.5 â†’ Leads to higher churn
2. **Level 2**: JobRole, OverTime status
3. **Level 3**: Age, Satisfaction metrics

---

### KNN: Hyperparameter Tuning
![KNN Tuning](11_knn_tuning.jpg)

**Finding:** Optimal K=11 provides best bias-variance tradeoff.
- K < 11: Overfitting risk
- K = 11: Sweet spot (84.95% CV accuracy)
- K > 11: Underfitting tendency

---

### KNN: Cross-Validation Scores
![KNN CV Scores](12_knn_cv_scores.jpg)

**Result:** Mean: 0.8513 (Â±0.0096) - Even more stable than Decision Tree!

---

### KNN: Confusion Matrix
![KNN Confusion Matrix](13_knn_confusion_matrix.jpg)

**Analysis:** Conservative approach - predicts retention for almost everyone (high accuracy but misses churners).

---

### Model Comparison: ROC Curves
![Model Comparison ROC](14_model_comparison_roc.jpg)

**Visual Comparison:**
- Green line (KNN): Curves above blue line across all thresholds
- Blue line (DT): Lower curve, less discriminative
- KNN shows **superior overall discrimination**

---

### Model Comparison: All Metrics
![Model Comparison All](15_model_comparison.jpg)

**Key Insight:** KNN wins on accuracy, precision, and ROC-AUC. Decision Tree wins on recall and F1-score.

---

## ğŸ’¡ Business Insights

### High-Risk Employee Profile
- **Age:** 25-35 years (early career)
- **Tenure:** <2 years with company
- **Income:** <$2,500/month (bottom quartile)
- **Work Status:** Regular overtime workers
- **Benefits:** Limited or no stock options

### Churn Drivers (Top 5)
1. **Low Experience** (Total Working Years < 1.5)
2. **Low Compensation** (Monthly Income < $2,361)
3. **Young Age** (Age < 28.5)
4. **Overtime Work** (Working >0.5 hours/week overtime)
5. **Limited Benefits** (Stock Option Level < 1)

### Cost-Benefit Analysis
- **Current Annual Churn:** 237 employees (16.1%)
- **Annual Turnover Cost:** ~$3.08M (assuming $50k salary, 200% cost multiplier)
- **Target After Interventions:** 12-14% churn rate
- **Expected Annual Savings:** $1.2M - $2.0M
- **ROI:** Pays for itself with 24-40 saved departures

---

## ğŸ¯ Usage Guide

### Running the Complete Pipeline

#### Option 1: Run All Notebooks Sequentially
```bash
cd notebooks/

# 1. Data Exploration & Visualization
jupyter notebook 1_data_exploration.ipynb

# 2. Decision Tree Training & Evaluation
jupyter notebook 2_decision_tree_analysis.ipynb

# 3. KNN Training, Tuning & Model Comparison
jupyter notebook 3_knn_analysis.ipynb
```

#### Option 2: Use Saved Models for Predictions
```python
import pickle
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load trained models
with open('../results/models/decision_tree_model.pkl', 'rb') as f:
    dt_model = pickle.load(f)

with open('../results/models/knn_model.pkl', 'rb') as f:
    knn_model = pickle.load(f)

# Load scaler (for KNN)
with open('../results/models/scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# Load new employee data
new_employees = pd.read_csv('../data/new_employees.csv')

# Decision Tree predictions
dt_predictions = dt_model.predict(new_employees)
dt_probabilities = dt_model.predict_proba(new_employees)[:, 1]

# KNN predictions (with scaling)
new_employees_scaled = scaler.transform(new_employees)
knn_predictions = knn_model.predict(new_employees_scaled)
knn_probabilities = knn_model.predict_proba(new_employees_scaled)[:, 1]

# Identify high-risk employees (churn probability > 0.6)
high_risk_dt = new_employees[dt_probabilities > 0.6]
high_risk_knn = new_employees[knn_probabilities > 0.6]
```

### Making Predictions on New Data

```python
# Prepare data in same format as training
new_data = pd.read_csv('new_employees.csv')

# Scale features (KNN requires this)
new_data_scaled = scaler.transform(new_data)

# Get predictions
churn_prob = knn_model.predict_proba(new_data_scaled)[:, 1]

# Add predictions to dataframe
new_data['Churn_Risk_Score'] = churn_prob
new_data['Churn_Prediction'] = knn_model.predict(new_data_scaled)

# Flag high-risk employees for HR intervention
new_data['Intervention_Required'] = churn_prob > 0.6
```

---

## ğŸ“‹ Results & Recommendations

### Recommended Model: **KNN**

**Why KNN is recommended for production:**
1. **Highest Accuracy:** 84.81% vs Decision Tree's 80.05%
2. **Best Discrimination:** ROC-AUC 0.7494 vs 0.5580
3. **Higher Precision:** 70% means fewer false alarms
4. **Stable Performance:** Lower CV variance (0.96% vs 1.30%)

### Business Recommendations

#### Immediate Actions (0-30 days)
1. **Deploy KNN Model** - Integrate into HR dashboard
2. **Compensation Review** - Audit salaries <$2,500/month
3. **Early Tenure Programs** - Enhance first-year experience

#### Short-Term (1-3 months)
1. **Work-Life Balance** - Cap overtime, offer flexibility
2. **Stock Options Expansion** - Increase eligibility
3. **Mentorship Programs** - Target younger employees

#### Long-Term (3+ months)
1. **Automated Risk Scoring** - Quarterly churn risk updates
2. **Intervention Tracking** - Measure program effectiveness
3. **Model Retraining** - Quarterly with new data

### Expected Impact
- **Churn Reduction:** 15-25% decrease in attrition
- **Cost Savings:** $1.2M - $2.0M annually
- **ROI:** Positive with just 24-40 prevented departures
- **Strategic Benefit:** Data-driven HR strategy

---

## ğŸ› ï¸ Tech Stack

**Core Libraries:**
- **pandas** - Data manipulation & analysis
- **numpy** - Numerical computing
- **scikit-learn** - Machine learning models
- **matplotlib** - Static visualizations
- **seaborn** - Statistical visualizations
- **jupyter** - Interactive notebooks

**Environment:**
- Python 3.8+
- Virtual environment recommended
- No GPU required (CPU sufficient)

**Reproducibility:**
- `random_state=42` set throughout
- Relative paths for data/results
- Stratified cross-validation
- Feature scaling (StandardScaler)

---

## ğŸ“ Requirements

Create `requirements.txt`:
```
pandas==1.3.0
numpy==1.21.0
scikit-learn==1.0.0
matplotlib==3.4.2
seaborn==0.11.1
jupyter==1.0.0
```

Install all dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“§ Contact & Support

**Author:** Adarsh Ramakrishna  
**Email:** adireland.ie@gmail.com  
**GitHub:** [@adarsh-ramakrishna](https://github.com)  
**LinkedIn:** [Adarsh Ramakrishna](https://linkedin.com/in/adarsh-ramakrishna)

For questions or support, please open an issue on GitHub.

---

## ğŸ”— References & Resources

- **Dataset:** [IBM HR Analytics - Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)
- **Scikit-learn Documentation:** [sklearn.org](https://scikit-learn.org/)
- **Cross-Validation Guide:** [Scikit-learn CV](https://scikit-learn.org/stable/modules/cross_validation.html)
- **Feature Importance:** [Interpreting Feature Importance](https://scikit-learn.org/stable/modules/inspection.html)

---

## ğŸ“Š Project Statistics

- **Total Employees Analyzed:** 1,470
- **Features Engineering:** 34 â†’ 51
- **Models Developed:** 2 (Decision Tree + KNN)
- **Visualizations Created:** 15+
- **Cross-Validation Folds:** 5
- **Hyperparameters Tuned:** 7+
- **Training Time:** <5 minutes
- **Prediction Time:** <10ms per employee

---

## ğŸ“ Learnings & Insights

### Key Technical Learnings
âœ… Importance of stratified sampling for imbalanced datasets  
âœ… Feature scaling critical for distance-based algorithms (KNN)  
âœ… Trade-offs between model interpretability vs accuracy  
âœ… Cross-validation for robust performance estimation  
âœ… Hyperparameter tuning for optimal model selection  

### Business Learnings
âœ… First 2 years crucial for retention  
âœ… Compensation directly impacts churn  
âœ… Overtime correlates with higher attrition  
âœ… Age and experience are strong predictors  
âœ… Early intervention more cost-effective than reactive approach  

---

## â­ Acknowledgments

Special thanks to:
- IBM for the HR Analytics dataset
- Kaggle community for insights
- Scikit-learn team for excellent ML library
- TUS Athlone for academic guidance

---

<div align="center">

### Made with â¤ï¸ by Adarsh Ramakrishna

â­ If this project helps you, please star the repository!

[â¬† Back to Top](#-employee-churn-prediction---decision-tree-vs-knn)

</div>
